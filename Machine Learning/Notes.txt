Supervised - Classification (For Binary values), Regression (For continous values)
Unsupervised - Clustering, Dimensionality Reduce

Classification - KNN, Descision Trees, Logistic Regression, Random forest, Naive Bayes
Regression - Linear regression

Clustering - Kmeans, hierarchical, Density based, Gaussian Mixture, DBScan
Dimensionality Reduce - PCA (Principal Component Analysis), SVD (Singular Value Decomposition)

p-value -
The p-value only tells you how likely the data you have observed is to have occurred under the null hypothesis. 
If the p-value is below your threshold of significance (typically p < 0.05), then you can reject the null hypothesis, 
but this does not necessarily mean that your alternative hypothesis is true.
higher the p-value, lower the importance of a feature
-- It is sum of 3 parts - (1. The probability random chance would result in observation)
(2. The probability of observing something else which is equally rare)
(3. The probability of observing something rarer or more extreme)

Random Forest -
To create Trees-
1. Using bootstrapped method create a dataset
2. Only considering a random subset of variables at each step
3. Do this for n number of times
4. Use the out-of-bag dataset to see the accuracy
5. We change the number of variables used in step 2 and see which gives us the most accuracy 
Bagging - 'B'ootstrapping data plus using 'AGG'regating to make decision is Bagging
Out-of-bag dataset - data which didnt make it to bootstrap dataset
Missing values -
For missing values we take Mode, Median of values with similar outputs
Then we find out similarity between each rows using Proximity matrix
For each output of tree whenever any row falls under same category we add 1 to its corresponding row & col in matrix
Once all iterations are done we divide each value of matrix with number of trees
Then based on the Proximity matrix we derive values for the missing values using probabilities
-- Proximity matrix is the closeness between different data when processed through different trees.
-- 1 - Proximity matrix is distance matrix which shows distance between 2 or more data

ADABoost - 
Stump (weak-learners) - a tree with only 1 root and 2 leaves nodes
Usually in Random forest each tree is equally worthy of other trees, but in ADABoost we use stumps(weak learners) and there some stumps which have more say than others
Each stump is made by taking previous stump's mistakes
Learning rate is used to control loss function for calculating the weight of base model.

Descision matrix - 
when matrix is 2x2
-- sensitivity = True Positive / (True Positive + False negative)
-- specificity = True negative / (True negative + False Positive)
-- precision = TP / (TP + FP)
-- recall = TP / (TP + FN)
when matrix is NxN 
-- sensitivity(1) = True Positive(1) / (True Positive(1) + False negative(1) + .. + False negative(N))
-- sensitivity(2) = True Positive(2) / (True Positive(2) + False negative(1) + .. + False negative(N)).......
-- specificity(1) = True negative(1) / (True negative(1) + False Positive(1) + .. + False Positive(N))
-- specificity(2) = True negative(2) / (True negative(2) + False Positive(1) + .. + False Positive(N)).......

T-Test & Annova uses Design Matrics to eliminate the residuals

Linear Regression - (coefficient = slope, constant = intercept)
Bias - the inability to capture the true relationship 
Variance - the difference in the prediction of the model when using different datasets 
-- Overfit - when the squiggly line fits TRAINING set well, but not the test set it is Overfit
-- regularisation, bagging, boosting
-- Bagging (bootstrap aggregating) is an ensemble method that involves training multiple models independently on random subsets of the data, and aggregating their predictions through voting or averaging.

Least Square method is used to calulate the best fit line, in which sum of sqaure residual is least
-- In this we take the a straigth line parallel to x axis and rotate it to get the least square value

R square (means Residual square) = (SS(mean) - SS(fit)) / SS(mean)  [SS is Sum of sqaures]
p-value in linear regression tells if the coefficient is near to 0 or not 
p-value is 2 if there are 2 parameters(X, Y), it will be 1 if there is only parameter(y) so on and so forth 
coefficient tells the direction of line. If positive the line starts at 0 to infinity, if negative it starts from high and then decline, if 0 it is straigth line parallel to x axis 
coefficient could be compared as angle 

K-means clustering -
Elbow plot helps us in figuring out the most efficient number of clusters
-- Centroid based - in which new point is predicted based on the distance from Centroid
-- Single linkage - in which new point is predicted based on the distance from Closet point in the cluster
-- Complete linkage - in which new point is predicted based on the distance from farthest point in the cluster
-- Average linkage - in which new point is predicted based on the average distance of all the points

Hierarchical clustering -
We find similarity(mostly using Euclidean distance) between 2 rows and map it together and keep doing that until all rows are clustered
Height of the dendogram tells us the similarity between 2 cluster, smaller Height means more similar and vice versa

DBScan (Density based scan)- 
- we take all points randomly and create a circle of some radius around each point and see how many points it overlaps
- we fix a value of overlapping points like 4 in our case and all the points which overall 4 points are called as Core points and remaining are non core points
- we take 1 core point and all the other core points overlapping that point is added to the cluster
- Once all the core points are added to clusters, we check if any non core point overlaps core point
- if yes we add it to the cluster, if not we dont add it to cluster
- if any non core point falls in the range of 2 different cluster core point, we add it to cluster whichever we started first with

Decision Tree- 
Root Node --> Internal nodes/ branches --> leaf node
-- When it classifies any object it is a classification tree
-- When it predicts any values it is a regression tree

Classification Decision tree -
Impure - When leaf has values for both Yes & No, it is called as impure
Gini impurity of 1 leaf = 1 - square(probability of Yes) - square(probability of No)
Total Gini impurity is the total weighted average of the leaves
i.e. Total Gini Impurity = ((count of leaf 1)/(count of Leaf 1 & 2))*Gini impurity of leaf 1 + ((count of leaf 2)/(count of Leaf 1 & 2))*Gini impurity of leaf 2
In case of continous values we will sort the column and take the average of adjacent rows and find gini impurity based on it.
Lowest Gini impurity column becomes the root node and we continue till there are no impure nodes which become leaf nodes
If a node has more impurity than existing we can ignore that node to avoid overfit. 
-- Missing data - 1. We can take Mean/Median in case of continous values or Mode in case of non-continous
2. we can find another column with highest correlation and predict using linear regression in case of continous values or in case of non linear select values from corresponding column 

In case of smaller dataset use Entropy or else use Gini impurity since there is log in entropy it is slower

Regression Tree -
- Take 2 points in graph and calculate their Squared residuals by taking averages
- Take sum of Squared residuals(SSR) for all points and plot a graph (Similar to Elbow plot in K-means)
- Value with least Sum of Squared residuals will be root, then split the points on the left and right and do the whole process again to find internal nodes until leaf nodes
- We can have minimum number of points to allow for splits, to avoid overfitting (Usually 20) - which means if a node has less than 20 points we will not split it
In case of multiple columns we find the lowest SSR and make that as the root and follow same process like above.

Pruning Regression Trees -
Pruning is done to avoid overfitting data
Steps for Cost complexity Pruning -
1. create sub-trees by reducing each node in an iteration
2. To find out sum of sqaured residuals(SSR) of each sub tree, by adding SSR of each leaf
3. calculate tree score of each tree
-- Tree score = SSR + (Alpha * Terminal)
-- Tree complexity penalty = (Alpha * Terminal)
-- Terminal is the number of leaves
-- Alpha is a constant value calculated by using cross validation
4. pick the tree with lowest Tree score

Gradient Boost - Regression
1. Start with a tree with only 1 leaf which has avg value of the variable to be predicted
2. Create & add a tree with taking residuals as reference and using some learning rate(between 0 & 1)
3. Keep adding trees taking new residuals into conisderation based on errors made by previous tree

XGBoost -
Uses similarity scores to calculate the residuals
similarity score = (Sum of the Sqaure of residuals)/(Number of points + Lambda)
then we find the gain by = similarity score of left + right - root
More the similarity score better the algorithm

SVM & SVC-
Margin - Distance between hyperplane and nearest point is called margin
Hyperplane - The threshold which is used to divide the 2 set of points
Points are called as vectors
Support Vectors - the nearest points from hyperplane are called as support vectors
We need to have the maximum distance between the 2 support vectors (Maximal margin classifier)
Soft margins - when we allow misclassifications, distance between observations and the threshold is called soft margin
SVC (Support vector classifier) - when we use normal dataset(unmodified dataset) to classify the data using plane
SVM (Support vector Machine) - when we use kernel functions to sysematically find SVC
Kernel trick - The kernel function doesnt transform the data in higher dimesnions, rather it just does teh calculation
Radial Basis function SVM (RBF SVM) - plots data in infinite dimensions and works as weighted nearest neighbour model (it is preferred when the data is linearly non separable)

Naive Bayes - 
We calculate probabilities for each values using Bayes Theorem and based on that we classify
We add 1 box to every possibilty which is called as blackbox to avoid getting a 0 probability

Gaussian Naive Bayes-
In this we calulate the Mean and Standard deviation for each column.
Then we plot Gaussian Distribution for each dataset(column-wise).
Then we find the prior probability
To Predict any value we take each probabilities and add their log values(to avoid underflow) and see which is higher
--Underflow - when any value is very close to 0 
--Prior probability - the initial probability based on taining data is called prior probability

Regularisation -
To add some amount of penalty
Ridge Regression - in this the penalty is added based on the a lambda value multiplied by Sqaure of slope
Lasso Regression - in this the penalty is added based on the a lambda value multiplied by Absolute of slope
Elastic Regression - it is combination of Ridge & Lasso you add (Lambda1 + Absolute of each variable) + (Lambda2 + Square of each variable)

Dimension Reduction-
PCA -
PCA reduces dimensions based on the most variations 

LDA -
LDA is used for maximising the seperatibility between multiple categories, like PCA it creates a axis which maximises the seperatibility
LDA categories - 1. 'Maximise' the distance between means  2. 'Minimise' variation (represented as scatter)

MDS - 
MDS(Multi-dimensional Scaling) - has 2 types metric/Classical(Also called as PCoA - Principal Coordinate Analysis) and Non-metric
PCoA reduces dimensions based on the distance between 2 or more columns rest eveything is similar to PCA
There are multiple ways to find distance between 2 columns like Euclidean distance, log fold distance, Manhattann, Hamming

t-SNE -
t-SNE - t-distributed Stochastic Neighbor Embedding 
It finds a way to project data into a low dimensional space so that the clustering in high dimension is preserved
For ex - in 2D graph there are 3 clusters, this algo will take all points from cluster and randomly plot them on a linear graph
Then each point will move closer to its similar points until all the similar values are close to each other

UMAP -
Uniform Manifold Approximation and Projection
It reduces dimensions based on the similarity scores and forms cluster as similar approach like t-SNE 
Difference is t-SNE randomly plots points on linear graph whereas UMAP uses Spectral Embedding to plot on liner graph
Also t-SNE moves only 1 point in each iteration whereas UMAP moves more than 1 point in each iteration based on similarity score

For all algo -
ROC - Reciever Operator Characteristic graph provide a simple way to summarize multiple Descision matrix info
AUC - Area under the curve 
If AUC of 1 algo covers more area than the AUC of another then that algo is better
Surpise - Suprise is inverse of log of probability 
Surpise = log( 1/ p(x) )
Entropy - If we calculate Surpise for 100 times and find out Surprise for each time that is called as Entropy
Entropy = Summation( p(x) * log(1/ p(x)) ) 
Entropy is closer to value which has higher probability or p(x)
Entropy in laymans term is the measure of disorder or impurity in the given dataset.
p-value is less than threshold (usually 0.05) then we can reject the null hypothesis 
The central limit theorem says that the sampling distribution of the mean will always be normally distributed, as long as the sample size is large enough
Cross-validation - creates multiple random splits of the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits.
Bias - the inability to capture the true relationship 
Variance - the difference in the prediction of the model when using different datasets
Data Leakage - it happens when the training data contains information about the target
Central limit Theorem - No matter whichever distribution we start with their means will be a normal distribution.

Bias -
The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. 
Being high in biasing gives a large error in training as well as testing data. 
It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. 
By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. 
Such fitting is known as the Underfitting of Data.

Variance -
The variance measures the average degree to which each point differs from the mean.
The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. 
The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before.
As a result, such models perform very well on training data but have high error rates on test data. 
When a model is high on variance, it is then said to as Overfitting of Data. 
Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high. 
While training a data model variance should be kept low.

Overfit - High variance, low bias - when our model does well with training data but not any other data
Underfit - low variance, high bias - when our model still has scope of improvement in the training data

Target encoding -
Data Leakage - it happens when the training data contains information about the target
It can lead to data leakage which leads to overfit models
to avoid that we can use K Fold target Encoding


Deep Learning -

Neural Networks - 

Activation Functions - Curved or bent lines are called activation functions

Different types of activation functions - 
1. Soft Plus - in which we use curved lines
2. ReLU (Rectified Linear Unit) - usually a bent line
3. Sigmoid - s shaped curved line where value between 0 and 1
4. Softmax
5. Tanh - similar to sigmoid but value lies between -1 to 1
6. Linear

Hidden layers - Nodes in the middle of input & output nodes are called hidden layers
Usually Hit n trial methods is used to confirm the number of hidden layers

Weights - the parameters we multiply in hidden layers are Weights
Biases - the parameters we add in hidden layers are Biases

Loss Function -

the Loss function is a method of evaluating how well your algorithm is modeling your dataset. It is a mathematical function of the parameters of the machine learning algorithm.

Different loss functions for Regression-
1. MSE (Mean Squared Error/Squared loss/ L2 loss) - disadvantage is - it doesnt handle outliers since we are squaring
2. MAE (Mean Absolute Error/ L1 loss Functions) - disadvantage is - Gradient descent cannot be applied directly as graph is not differentiable 
3. Huber loss - Apply MSE when the actual and prediced value is less than a parameter(Delta) else apply MAE

Different loss functions for Classification-
1. Binary Cross Entropy/log loss Functions in machine learning models
2. Categorical Cross Entropy


When to use which Loss & Activation function -

Always use RelU in hidden layers

For Classification -
1. Binary Classification - output layer use sigmoid and loss function Binary Cross Entropy
2. MultiClass Classification - output layer use softmax and loss function Categorical Cross Entropy
When to use categorical cross-entropy and sparse categorical cross-entropy?
If target column has One hot encode to classes like 0 0 1, 0 1 0, 1 0 0 then use categorical cross-entropy. and if the target column has Numerical encoding to classes like 1,2,3,4….n then use sparse categorical cross-entropy.

For Regression -
output layer use linear and loss function can be MSE, MAE, Huber loss

Prompt engineering - (https://medium.com/@bryan.mckenney/teaching-llms-to-think-and-act-react-prompt-engineering-eef278555a2e)
https://www.youtube.com/watch?v=1c9iyoVIwDs&ab_channel=IBMTechnology
- RAG(Retireval Augmented Generator) (Improve domain knowledge) - usually adding a retierver which gets data from a set of data or database (Used to overcome Incorrect, Outdated information and adding source to the content)
- COT(Chain of thought) (to get precise output) - to break down the prompt/steps and get more precise answer and improvise the result. Promoting ideas using 'thoughts'  in the form of chunks one by one to obtain actual answers. Language models arrive at your desired answers through reasoning and logic.
- ReAct(Thought, Action, and Observation) - Different from the chain of thoughts, this involves both private knowledge base (db) and public language model (llm) data. If information isn't in the knowledge base, it goes back to the public llm data (trained data) for results.
- DSP(Direct Stimulus Prompting)- The latest method involves hinting the prompt with a specific hint to get the answers.

Few-shot learning - COT, ReAct, DSP


Hallucination(https://www.youtube.com/watch?v=cfqtFvWOfg0&ab_channel=IBMTechnology) -

What is hallucinating - 
- Sentence Contradiction - when it contradicts previous response sentences like - sky is blue today, sky is green today
- Prompt Contradiction - when it contradicts to the prompt used to generate it
- Factual Contradiction - when it predicts incorrect facts
- Non-sensical - when it predicts non-sense data like: Paris is capital of France, Paris is a famous singer

NLP -

Encoders - usually are for NLU(Natural language Understanding)
Decoders - usually are for NLG(Natural Language Generation)

Each of these parts can be used independently, depending on the task:

Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.
Decoder-only models: Good for generative tasks such as text generation.
Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.

Transformers have Flexibility: At their core, all models are simple PyTorch nn.Module or TensorFlow tf.keras.Model classes and can be handled like any other models in their respective machine learning (ML) frameworks.

Logits - its the raw predicted data without applying any activation function like Softmax or Argmax