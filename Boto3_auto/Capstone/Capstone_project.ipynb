{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'WXMVNNPYJ3J13T19',\n",
       "  'HostId': 'a1FmliAvblDWOE7DQQjwiT1APaqpBWL14N7XdwjNFDyvb1fIcU/zmwbQAPom8HIARR8Y0U5b6GQ=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'a1FmliAvblDWOE7DQQjwiT1APaqpBWL14N7XdwjNFDyvb1fIcU/zmwbQAPom8HIARR8Y0U5b6GQ=',\n",
       "   'x-amz-request-id': 'WXMVNNPYJ3J13T19',\n",
       "   'date': 'Thu, 11 Apr 2024 18:06:11 GMT',\n",
       "   'location': '/bucket-capstone-project',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Location': '/bucket-capstone-project'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3', 'us-east-1')\n",
    "s3_client.create_bucket(Bucket='bucket-capstone-project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create table Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'capstone-metadata'\n",
    "attributes = [\n",
    "    {\n",
    "        'AttributeName' : 'id',\n",
    "        'AttributeType' : 'S'\n",
    "    },\n",
    "    {\n",
    "        'AttributeName' : 'filetype',\n",
    "        'AttributeType' : 'S'\n",
    "    },\n",
    "    {\n",
    "        'AttributeName' : 'size',\n",
    "        'AttributeType' : 'N'\n",
    "    }\n",
    "]\n",
    "\n",
    "key_schema = [\n",
    "    {\n",
    "        'AttributeName' : 'id',\n",
    "        'KeyType' : 'HASH'\n",
    "    },\n",
    "    {\n",
    "        'AttributeName' : 'size',\n",
    "        'KeyType' : 'RANGE'\n",
    "    }\n",
    "]\n",
    "\n",
    "provisioned_throughput = {\n",
    "    'ReadCapacityUnits' : 5,\n",
    "    'WriteCapacityUnits' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create Dynamo Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamo_client = boto3.client('dynamodb','us-east-1')\n",
    "\n",
    "response = dynamo_client.create_table(\n",
    "    TableName = table_name,\n",
    "    AttributeDefinitions = attributes,\n",
    "    KeySchema = key_schema,\n",
    "    ProvisionedThroughput = provisioned_throughput,\n",
    "    GlobalSecondaryIndexes = [\n",
    "        {\n",
    "            'IndexName' : 'idx1',\n",
    "            'KeySchema' : [\n",
    "                {\n",
    "                    'AttributeName' : 'filetype',\n",
    "                    'KeyType' : 'HASH'\n",
    "                }\n",
    "            ],\n",
    "            'Projection' : {'ProjectionType' : 'ALL'},\n",
    "            'ProvisionedThroughput' : {\n",
    "                'ReadCapacityUnits' : 5,\n",
    "                'WriteCapacityUnits' : 5\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload Media function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def upload_to_s3(local_path, bucket_name):\n",
    "    key = Path(local_path).name\n",
    "    s3_client.upload_file(Filename=local_path,Bucket=bucket_name,Key=key)\n",
    "\n",
    "    return f\"{bucket_name}/{key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb = boto3.client('dynamodb','us-east-1')\n",
    "\n",
    "### Extract metadata from the file ###\n",
    "def extract_metadata(event):\n",
    "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    key = event['Records'][0]['s3']['object']['key']\n",
    "    size = event['Records'][0]['s3']['object']['size']\n",
    "    file_type = Path(key).suffix[1:]\n",
    "    if not file_type:\n",
    "        file_type = \"None\"\n",
    "    return bucket, key, file_type, size\n",
    "\n",
    "### Add metadata to database. Use file identifier as id ###\n",
    "def add_to_database(bucket, key, file_type, size):\n",
    "    id = f\"{bucket}/{key}\"\n",
    "    response = dynamodb.put_item(\n",
    "        TableName='MediaMetadata',  # Change to your table\n",
    "        Item={\n",
    "            'id': {'S': id},\n",
    "            'filetype': {'S': file_type},\n",
    "            'size': {'N': str(size)}\n",
    "        }\n",
    "    )\n",
    "    print(f\"Data added to DynamoDB: {response}\")\n",
    "\n",
    "### Lambda handler routine ###\n",
    "def lambda_handler(event, context):\n",
    "    # Extract bucket and file key from S3 Event\n",
    "    bucket, key, file_type, size = extract_metadata(event)\n",
    "    print(file_type, size / 1024)\n",
    "    add_to_database(bucket, key, file_type, size / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "iam_client = boto3.client('iam', region_name=\"us-east-1\")\n",
    "\n",
    "lambda_execution_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [  \n",
    "                \"s3:GetObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::*/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"dynamodb:PutItem\"  \n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:dynamodb:*:*:table/capstone-metadata\" \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "role_name = 'LambdaMetaDataTrigger'\n",
    "role_description = 'Role for Trigger Lambda'\n",
    "role_response = iam_client.create_role(\n",
    "    RoleName=role_name,\n",
    "    Description=role_description,\n",
    "    AssumeRolePolicyDocument=json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    ")\n",
    "\n",
    "policy_name = 'LambdaTriggerPolicy'\n",
    "iam_client.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=policy_name,\n",
    "    PolicyDocument=json.dumps(lambda_execution_policy)\n",
    ")\n",
    "\n",
    "role_arn = role_response['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::767398018810:role/LambdaMetaDataTrigger'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lambda.py\", \"r\") as f:\n",
    "    function_code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pathlib import Path\n",
      "import boto3\n",
      "\n",
      "dynamodb = boto3.client('dynamodb')\n",
      "\n",
      "### Extract metadata from the file ###\n",
      "def extract_metadata(event):\n",
      "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
      "    key = event['Records'][0]['s3']['object']['key']\n",
      "    size = event['Records'][0]['s3']['object']['size']\n",
      "    file_type = Path(key).suffix[1:]\n",
      "    if not file_type:\n",
      "        file_type = \"None\"\n",
      "    return bucket, key, file_type, size\n",
      "\n",
      "### Add metadata to database. Use file identifier as id ###\n",
      "def add_to_database(bucket, key, file_type, size):\n",
      "    id = f\"{bucket}/{key}\"\n",
      "    response = dynamodb.put_item(\n",
      "        TableName='MediaMetadata',  # Change to your table\n",
      "        Item={\n",
      "            'id': {'S': id},\n",
      "            'filetype': {'S': file_type},\n",
      "            'size': {'N': str(size)}\n",
      "        }\n",
      "    )\n",
      "    print(f\"Data added to DynamoDB: {response}\")\n",
      "\n",
      "### Lambda handler routine ###\n",
      "def lambda_handler(event, context):\n",
      "    # Extract bucket and file key from S3 Event\n",
      "    bucket, key, file_type, size = extract_metadata(event)\n",
      "    print(file_type, size / 1024)\n",
      "    add_to_database(bucket, key, file_type, size / 1024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(function_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_name = \"metadata\"\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "lambda_client = boto3.client('lambda', region_name='us-east-1')\n",
    "\n",
    "with io.BytesIO() as deployment_package:\n",
    "    with zipfile.ZipFile(deployment_package, 'w') as zipf:\n",
    "        zipf.writestr('lambda_function.py', function_code)\n",
    "\n",
    "    create_function_response = lambda_client.create_function(\n",
    "       FunctionName=function_name,\n",
    "       Runtime=\"python3.8\",\n",
    "       Role=role_arn,\n",
    "       Handler=\"lambda_function.lambda_handler\",\n",
    "       Code={\n",
    "           'ZipFile': deployment_package.getvalue()\n",
    "       }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ce14c1b8-1304-421c-8a36-b6aa12e0dd2a',\n",
       "  'HTTPStatusCode': 201,\n",
       "  'HTTPHeaders': {'date': 'Fri, 12 Apr 2024 18:50:21 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '315',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'ce14c1b8-1304-421c-8a36-b6aa12e0dd2a'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Statement': '{\"Sid\":\"metadata_trigger\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"s3.amazonaws.com\"},\"Action\":\"lambda:InvokeFunction\",\"Resource\":\"arn:aws:lambda:us-east-1:767398018810:function:metadata\",\"Condition\":{\"ArnLike\":{\"AWS:SourceArn\":\"arn:aws:s3:::bucket-capstone-project\"}}}'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_arn = \"arn:aws:s3:::bucket-capstone-project\"  # Change media-library-bucket-capstone to your bucket name\n",
    "lambda_client.add_permission(\n",
    "     FunctionName=function_name,\n",
    "     StatementId='metadata_trigger',  # Unique statement ID\n",
    "     Action='lambda:InvokeFunction',  # Allow to invoke the function\n",
    "     Principal='s3.amazonaws.com',  # \n",
    "     SourceArn=bucket_arn,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'E0F3WYKWZ5CS2SQS',\n",
       "  'HostId': 'ew/PH/pwfrnmvS1ckmJtZ++ilPvtqw1qz9LhIX71boT+6/EvMtuRVhOviqJ9O8L3ewcgEti3i1Y=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'ew/PH/pwfrnmvS1ckmJtZ++ilPvtqw1qz9LhIX71boT+6/EvMtuRVhOviqJ9O8L3ewcgEti3i1Y=',\n",
       "   'x-amz-request-id': 'E0F3WYKWZ5CS2SQS',\n",
       "   'date': 'Fri, 12 Apr 2024 18:52:49 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_configuration = {\n",
    "    'LambdaFunctionConfigurations': [\n",
    "        {\n",
    "            'LambdaFunctionArn': create_function_response[\"FunctionArn\"],\n",
    "            'Events': ['s3:ObjectCreated:*'],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Configure the S3 event trigger\n",
    "s3_client.put_bucket_notification_configuration(\n",
    "    Bucket=\"bucket-capstone-project\",\n",
    "    NotificationConfiguration=event_configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
